{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introduction</h1>\n",
    "\n",
    "In this project, I performed sentiment analysis using an unsupervised machine learning approach on data collected from the version control website GitHub.  I explored the distributions and relationships between projects commit comments sentiments, and project features like watchers, comments time of day and the repository programming language. Data wrangling steps are performed to prepare the data for analysis. Sentiment score calculations for each commit comment is calculated, in which it required a number of preprocessing and score estimation steps.\n",
    " <br><br>\n",
    "<b>-Technologies and services used:</b><br>\n",
    "  <br>1.Spark Python API (PySpark) \n",
    "  <br>To perform a scalable sentiment analysis\n",
    "  <br><br>2.Google BigQuery\n",
    "  <br>Access GitHub data using GHTorrent which is a “scalable, queriable, offline mirror of data offered through the Github REST API”.[4]\n",
    "  <br><a href=\"https://bigquery.cloud.google.com/dataset/ghtorrent-bq:ght\">Access Google BigQuery</a>\n",
    "  <br><br>3.Databricks\n",
    "  <br>Run Python and SQL notebooks(Jupyter) for the project. Databricks Is a web-based platform that offers Spark ready environments for data analytics \n",
    "  <br><a href=\"https://community.cloud.databricks.com\">Access Databricks</a>\n",
    "  <br><br>4.Python\n",
    "  <br>Run the program that processes the data and calculate sentiments\n",
    "  <br><br>5.SQL\n",
    "  <br>Manipulate the data on BigQuery and Databricks\n",
    "  <br><br>6.Python NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Description of the dataset</h1>\n",
    "<br> 1.Projects:<br> \n",
    "“Information about repositories. A repository is always owned by a user.The forked_from field is empty unless the project is a fork in which case it contains the id of the project the project is forked from.The deleted field means that the project has been deleted from Github.The updated_at field indicates when the last full update was done for this project”.[4]\n",
    "<br><br> \n",
    "2.Commits:<br>\n",
    "“Unique commits.Each commit is identified globally through its sha field. If the author or the committer has not configured his Github email address, no resolution to a user entry is possible. In that case, GHTorrent generates artificial users using the provided email in the Git commit author or committer fields. If the user then configures his Github account, GHTorrent will update the artificial user accordingly.The project_id field contains a link to the project that this commit has been first associated with. This might not be the project this commit was initially pushed to, e.g. in case the fork is processed before the parent. See project_commits. The project_id field may be null when the repository has been deleted at the time the commit is processed. This situation might happen when retrospectively processing pull requests for a repository and the repository which the pull request originates from has been deleted.”[4]\n",
    "<br><br> \n",
    "3.Commit comments:<br>\n",
    "“Code review comments on commits.These are comments on individual commits.”[4] \n",
    "<br><br> \n",
    "4.Watchers:<br>\n",
    "“Users that have starred (was watched) a project. The created_at field is only filled in accurately for starrings for which GHTorrent has recorded a corresponding event. Otherwise, it is filled in with the latest date that the corresponding user or project has been created.”[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1 >Data wrangling (Assess data structure)</H1>\n",
    "\n",
    "<br>\n",
    "1 .<b>To determine how to access the same field in multiple tables:</b><br> I verified the structure of each table in BigQuery and reviewed GHTorrent documentation to learn more about each field. For example, The id for each record in the project table is named id in the project table, repo_id in the watcher's table and project_id in the commits table. <br>  \n",
    "\n",
    "2 .<b>To verify the data encoding:</b><br>I read BigQuery documentation and it seems that BigQuery convert all data to UTF-8 encoding . Dates are stored in the UTC standard. <br>\n",
    "\n",
    "3 .<b>To examine the data semantics: </b> <br>I reviewed the relational database schema in GHTorrent and queried the tables to understand relationships between entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data wrangling (Explore the data)</h1>\n",
    "<br>\n",
    "1.<b>I verified when was the data collected </b> <br>\n",
    "The GHTorrent data dump used is the latest which was updated on 2018-04-01. Making my analysis current with the publicly available data in GitHub. <br><br>\n",
    "\n",
    "2.<b>I queried GHTorrent dataset on BigQuery to examine a number of distributions including:</b> <br>\n",
    " \n",
    "-<b>How many projects are there per each programming language:</b> <br>\n",
    "<pre>\n",
    "SELECT language, \n",
    "       Count(*) count \n",
    "FROM   [ghtorrent-bq:ght_2018_04_01.projects] \n",
    "GROUP  BY language \n",
    "ORDER  BY count DESC \n",
    "</pre>\n",
    "<br>Notice how many projects have no programming language, given that its possible they are for non programming related project. \n",
    "<br>\n",
    "<br>\n",
    "-<b>How many watchers are there per quarter per year:</b><br>\n",
    "<pre>\n",
    "SELECT Year(p.created_at)    year, \n",
    "       Quarter(p.created_at) quarter, \n",
    "       Count(*)              stars \n",
    "FROM   [ghtorrent-bq:ght_2018_04_01.projects] p \n",
    "       INNER JOIN [ghtorrent-bq:ght_2018_04_01.watchers] w \n",
    "               ON ( p.id = w.repo_id ) \n",
    "GROUP  BY year, \n",
    "          quarter \n",
    "ORDER  BY year ASC, \n",
    "          quarter ASC   \n",
    "          \n",
    "</pre>\n",
    "The trend here is that the number of watchers has steadily increased reaching the peak (482K watches) on 2015. The number of watchers dropped to 481K and down to 351K in 2017.  \n",
    "<br><br>\n",
    "-<b>how many projects are there per year:</b><br>\n",
    "<pre>\n",
    "SELECT Count(*), \n",
    "       Year(created_at) y \n",
    "FROM   [ghtorrent-bq:ght_2018_04_01.projects] \n",
    "GROUP  BY y \n",
    "ORDER  BY y DESC \n",
    "</pre>\n",
    "<br>\n",
    "The number of projects have been increasing in the last years. Note that for 2018 we are only considering first quarter of the year. \n",
    "<br><br>\n",
    "GHTorrent has a number of schemas in Google BigQuery (each correspond to a MySQL dump). The naming convention for these dumps is not consistent (ex, ght vs ght_20xx_0x_xx) , which led me to make a mistake and assume that the latest schema is the one named ght with no date attached to the name. After using the queries above I discovered my error and corrected my approach. <br><br>\n",
    "\n",
    "\n",
    "3.<b>I grouped the tables based on meaningful fields to search for interesting patterns</b> and noticed that some tables have missing critical information. For example, while joining the Projects tables with Watchers and grouping by programming languages (a field in the project table). I noticed that the second most watched programming language is null. During my sample selection phase, I filtered out all projects with an empty/null programming language. \n",
    "Example of my queries: \n",
    "<pre>\n",
    "SELECT Count(*) count, \n",
    "       p.language \n",
    "FROM   [ghtorrent-bq:ght_2018_04_01.watchers] w \n",
    "       INNER JOIN [ghtorrent-bq:ght_2018_04_01.projects] p \n",
    "               ON ( w.repo_id = p.id ) \n",
    "GROUP  BY p.language \n",
    "ORDER  BY count DESC  \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Data wrangling (Verify data quality)</H1><br>\n",
    "1.<b>I verified the timestamps </b>used in many of the tables and it turned out this format is not compatible with Spark. TimeStamps in BigQuery are stored in (yyyy-mm-dd HH:mm:ss.ssss UTC) while Spark is compatible with (yyyy-mm-dd hh:mm:ss).During my sample selection on Google BigQuery, I used the SQL function (STRFTIME_UTC_USEC) to convert the timestamp to the following format '%Y-%m-%d %H:%M:%S'. Also, BigQuery stores all timestamp in the Coordinated Universal Time standard UDC and this simplify the analysis\n",
    "\n",
    "2.<b>I also did a number of verification and correction steps to processes the comments text</b> which include: \n",
    "<ul>\n",
    "<li>Checking if the comment text is empty(null). While choosing my sample dataset I filtered out all these comments</li>\n",
    "<li>Determined the written language for each comment to remove non english comments. This was done using Langdetect package which is a standalone Language Identification tool accessible through https://pypi.org/project/langdetect/ </li>\n",
    "<li>Remove all punctuations from comments to prevent errors. I used a Python defined function to handle this </li>\n",
    "<li>All hyperlinks were removed from the comments because they don’t contribute to the sentiment analysis</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Data wrangling (Select a Sample Dataset)</H1>\n",
    "\n",
    "I selected a small sample from GHTorrent to perform the sentiment analysis on. My sample include projects in programming languages that have the most commit comments in GitHub. The sample includes 1000 project for each of the top 9 programming languages in terms of commit comments.  \n",
    "\n",
    "<b>The following is the query that helped <b>determine the top 9 programming languages</b> in terms of commit comments:</b> \n",
    "<pre>\n",
    "SELECT Count(*)   count, \n",
    "       p.language AS lan \n",
    "FROM   [ghtorrent-bq:ght_2018_04_01.projects] p \n",
    "       INNER JOIN [ghtorrent-bq:ght_2018_04_01.commits] c \n",
    "               ON ( p.id = c.project_id ) \n",
    "       INNER JOIN [ghtorrent-bq:ght_2018_04_01.commit_comments] cc \n",
    "               ON ( c.id = cc.commit_id ) \n",
    "GROUP  BY lan \n",
    "ORDER  BY count DESC \n",
    "</pre>\n",
    "\n",
    "<b>To select a sample form the project table I used this query and I created a new table in BigQuery named -199320:yourFolderName.projects</b>: \n",
    "<pre>\n",
    "SELECT id AS id, ,owner_id AS owner_id, language AS language FROM (SELECT p.id AS id, p.url AS url, p.owner_id AS owner_id, p.name AS name, p.description AS description, p.language AS language, p.created_at AS created_at, p.forked_from AS forked_from, p.deleted AS deleted, p.updated_at AS updated_at FROM [ghtorrent-bq:ght_2018_04_01.projects] p WHERE id IN (SELECT project_id FROM [ghtorrent-bq:ght_2018_04_01.commits] c INNER JOIN [ghtorrent-bq:ght_2018_04_01.commit_comments] cc ON ( c.id = cc.commit_id ) WHERE cc.body IS NOT NULL) AND language = 'JavaScript' AND p.forked_from IS NULL ORDER BY id ASC LIMIT 1000), (SELECT p.id AS id, p.url AS url, p.owner_id AS owner_id, p.name AS name, p.description AS description, p.language AS language, p.created_at AS created_at, p.forked_from AS forked_from, p.deleted AS deleted, p.updated_at AS updated_at FROM [ghtorrent-bq:ght_2018_04_01.projects] p WHERE id IN (SELECT project_id FROM [ghtorrent-bq:ght_2018_04_01.commits] c INNER JOIN [ghtorrent-bq:ght_2018_04_01.commit_comments] cc ON ( c.id = cc.commit_id ) WHERE cc.body IS NOT NULL) AND language = 'Java' AND p.forked_from IS NULL ORDER BY id ASC LIMIT 1000), (SELECT p.id AS id, p.url AS url, p.owner_id AS owner_id, p.name AS name, p.description AS description, p.language AS language, p.created_at AS created_at, p.forked_from AS forked_from, p.deleted AS deleted, p.updated_at AS updated_at FROM [ghtorrent-bq:ght_2018_04_01.projects] p WHERE id IN (SELECT project_id FROM [ghtorrent-bq:ght_2018_04_01.commits] c INNER JOIN [ghtorrent-bq:ght_2018_04_01.commit_comments] cc ON ( c.id = cc.commit_id ) WHERE cc.body IS NOT NULL) AND language = 'Python' AND p.forked_from IS NULL ORDER BY id ASC LIMIT 1000), (SELECT p.id AS id, p.url AS url, p.owner_id AS owner_id, p.name AS name, p.description AS description, p.language AS language, p.created_at AS created_at, p.forked_from AS forked_from, p.deleted AS deleted, p.updated_at AS updated_at FROM [ghtorrent-bq:ght_2018_04_01.projects] p WHERE id IN (SELECT project_id FROM [ghtorrent-bq:ght_2018_04_01.commits] c INNER JOIN [ghtorrent-bq:ght_2018_04_01.commit_comments] cc ON ( c.id = cc.commit_id ) WHERE cc.body IS NOT NULL) AND language = 'Ruby' AND p.forked_from IS NULL ORDER BY id ASC LIMIT 1000), (SELECT p.id AS id, p.url AS url, p.owner_id AS owner_id, p.name AS name, p.description AS description, p.language AS language, p.created_at AS created_at, p.forked_from AS forked_from, p.deleted AS deleted, p.updated_at AS updated_at FROM [ghtorrent-bq:ght_2018_04_01.projects] p WHERE id IN (SELECT project_id FROM [ghtorrent-bq:ght_2018_04_01.commits] c INNER JOIN [ghtorrent-bq:ght_2018_04_01.commit_comments] cc ON ( c.id = cc.commit_id ) WHERE cc.body IS NOT NULL) AND language = 'PHP' AND p.forked_from IS NULL ORDER BY id ASC LIMIT 1000), (SELECT p.id AS id, p.url AS url, p.owner_id AS owner_id, p.name AS name, p.description AS description, p.language AS language, p.created_at AS created_at, p.forked_from AS forked_from, p.deleted AS deleted, p.updated_at AS updated_at FROM [ghtorrent-bq:ght_2018_04_01.projects] p WHERE id IN (SELECT project_id FROM [ghtorrent-bq:ght_2018_04_01.commits] c INNER JOIN [ghtorrent-bq:ght_2018_04_01.commit_comments] cc ON ( c.id = cc.commit_id ) WHERE cc.body IS NOT NULL) AND language = 'HTML' AND p.forked_from IS NULL ORDER BY id ASC LIMIT 1000), (SELECT p.id AS id, p.url AS url, p.owner_id AS owner_id, p.name AS name, p.description AS description, p.language AS language, p.created_at AS created_at, p.forked_from AS forked_from, p.deleted AS deleted, p.updated_at AS updated_at FROM [ghtorrent-bq:ght_2018_04_01.projects] p WHERE id IN (SELECT project_id FROM [ghtorrent-bq:ght_2018_04_01.commits] c INNER JOIN [ghtorrent-bq:ght_2018_04_01.commit_comments] cc ON ( c.id = cc.commit_id ) WHERE cc.body IS NOT NULL) AND language = 'C++' AND p.forked_from IS NULL ORDER BY id ASC LIMIT 1000), (SELECT p.id AS id, p.url AS url, p.owner_id AS owner_id, p.name AS name, p.description AS description, p.language AS language, p.created_at AS created_at, p.forked_from AS forked_from, p.deleted AS deleted, p.updated_at AS updated_at FROM [ghtorrent-bq:ght_2018_04_01.projects] p WHERE id IN (SELECT project_id FROM [ghtorrent-bq:ght_2018_04_01.commits] c INNER JOIN [ghtorrent-bq:ght_2018_04_01.commit_comments] cc ON ( c.id = cc.commit_id ) WHERE cc.body IS NOT NULL) AND language = 'C' AND p.forked_from IS NULL ORDER BY id ASC LIMIT 1000), (SELECT p.id AS id, p.url AS url, p.owner_id AS owner_id, p.name AS name, p.description AS description, p.language AS language, p.created_at AS created_at, p.forked_from AS forked_from, p.deleted AS deleted, p.updated_at AS updated_at FROM [ghtorrent-bq:ght_2018_04_01.projects] p WHERE id IN (SELECT project_id FROM [ghtorrent-bq:ght_2018_04_01.commits] c INNER JOIN [ghtorrent-bq:ght_2018_04_01.commit_comments] cc ON ( c.id = cc.commit_id ) WHERE cc.body IS NOT NULL) AND language = 'Objective-C' AND p.forked_from IS NULL ORDER BY id ASC LIMIT 1000)\n",
    "</pre>\n",
    "(9,000 records returned)<br>\n",
    "\n",
    "<b>To select the watcher I used the following query:</b>\n",
    "<pre>\n",
    "SELECT repo_id,\n",
    "       user_id,\n",
    "       STRFTIME_UTC_USEC(created_at, '%Y-%m-%d %H:%M:%S') AS created_at\n",
    "FROM [ghtorrent-bq:ght_2018_04_01.watchers]\n",
    "WHERE repo_id IN\n",
    "    (SELECT id\n",
    "     FROM [yourFolderName-199320:yourFolderName.projects])\n",
    "</pre>\n",
    "(7,192,843 records returned)<br>\n",
    "\n",
    "<b>To select the commits I used the following: </b>\n",
    "<pre>\n",
    "SELECT id,\n",
    "       author_id,\n",
    "       committer_id,\n",
    "       project_id,\n",
    "       STRFTIME_UTC_USEC(created_at, '%Y-%m-%d %H:%M:%S') AS created_at\n",
    "FROM [ghtorrent-bq:ght_2018_04_01.commits]\n",
    "WHERE project_id IN\n",
    "    (SELECT id\n",
    "     FROM [yourFolderName-199320:yourFolderName.projects])\n",
    "  AND id IN\n",
    "    (SELECT c.id\n",
    "     FROM [ghtorrent-bq:ght_2018_04_01.commits] c\n",
    "     INNER JOIN [ghtorrent-bq:ght_2018_04_01.commit_comments] cc ON (c.id = cc.commit_id)\n",
    "     WHERE cc.body IS NOT NULL)\n",
    "</pre>\n",
    "(97,847 records returned)<br><br>\n",
    "<b>To select the commit comments I used: </b>\n",
    "<pre>\n",
    "SELECT\n",
    "  id,\n",
    "  commit_id,\n",
    "  user_id,\n",
    "  body,\n",
    "  comment_id,\n",
    "  STRFTIME_UTC_USEC(created_at, '%Y-%m-%d %H:%M:%S') AS created_at\n",
    "FROM [yourFolderName-199320:yourFolderName.commit_comments]\n",
    "\n",
    "</pre>\n",
    "(254,963 records returned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Importing the dataset</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sh wget -P/FileStore/tables https://storage.googleapis.com/yourFolderName/watchers.csv\n",
    "#downloading the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sh wget -P/FileStore/tables https://storage.googleapis.com/yourFolderName/commits.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sh wget -P/FileStore/tables https://storage.googleapis.com/yourFolderName/projects.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sh wget -P/FileStore/tables https://storage.googleapis.com/yourFolderName/commit_comments_v2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls('file:/FileStore/tables/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Load the data in PySpark</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import  *\n",
    " \n",
    "#create schema for each file, to avoid wrong conversions  \n",
    "projects_schema = StructType([StructField('id', IntegerType(), False)\\\n",
    "                  ,StructField('owner_id', IntegerType(), False)\\\n",
    "                  ,StructField('language', StringType(), False)])\n",
    "\n",
    "watchers_schema = StructType([StructField('repo_id', IntegerType(), False)\\\n",
    "                  ,StructField('user_id', IntegerType(), False)\\\n",
    "                  ,StructField('created_at', TimestampType(), False)])\n",
    "\n",
    "commits_schema = StructType([StructField('id', IntegerType(), False)\\\n",
    "                 ,StructField('author_id', IntegerType(), False)\\\n",
    "                 ,StructField('committer_id', IntegerType(), False)\\\n",
    "                 ,StructField('project_id', IntegerType(), False)\\\n",
    "                 ,StructField('created_at', TimestampType(), False)])\n",
    "\n",
    "commit_comments_schema = StructType([StructField('id', IntegerType(), False)\\\n",
    "                         ,StructField('commit_id', IntegerType(), False)\\\n",
    "                         ,StructField('user_id', IntegerType(), False)\\\n",
    "                         ,StructField('body', StringType(), False)\\\n",
    "                         ,StructField('comment_id', IntegerType(), False)\\\n",
    "                         ,StructField('created_at', TimestampType(), False)])\n",
    "\n",
    "#enforce the schema above and create the dataframes\n",
    "projects_df = spark.read.csv('file:/FileStore/tables/projects.csv', header=True, schema=projects_schema)\n",
    "watchers_df = spark.read.csv('file:/FileStore/tables/watchers.csv', header=True, schema=watchers_schema)\n",
    "commits_df = spark.read.csv('file:/FileStore/tables/commits.csv', header=True, schema=commits_schema)\n",
    "commit_comments_df = spark.read.csv('file:/FileStore/tables/commit_comments_v2.csv', header=True, schema = commit_comments_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined  the structure of each dataset using schemas and then created the spark dataframes for each dataset (project , watchers , commits and commit_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "projects_df.printSchema()\n",
    "watchers_df.printSchema()\n",
    "commits_df.printSchema()\n",
    "commit_comments_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create the tables\n",
    "projects_df.createOrReplaceTempView(\"projects\")\n",
    "commits_df.createOrReplaceTempView(\"commits\")\n",
    "watchers_df.createOrReplaceTempView(\"watchers\")\n",
    "commit_comments_df.createOrReplaceTempView(\"commit_comments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a temporary table for each dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Preprocessing and Sentiment Calculation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sh pip install nltk\n",
    "%sh pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import all the packages need to preprocess the data and calculate the sentiment\n",
    "import pyspark\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the needed Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Download the needed corpus \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the needed corpus (resources) from NLTK to perfume many functions like lemmatization, stopword removal and sentiment calculation. \n",
    "\n",
    "'Averaged_perceptron_tagger' is used to determine the part of speech of word in a sentence.   \n",
    "'Wordnet' is used to lemmatize words and determine their lemma.  \n",
    "'Sentiwordnet' help in calculating the sentiment of each word.\n",
    "'Punkt' is used to perform sentence tokenization and the 'Stopwords' lists each stopwords to remove from sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code examples used:\n",
    "#1- https://github.com/dreyco676/nlp_spark/blob/master/preproc.py\n",
    "#2- https://github.com/dreyco676/nlp_spark/blob/master/nlp_with_spark.ipynb\n",
    "\n",
    "#determine the language\n",
    "def check_lang(data_str):\n",
    "    try:\n",
    "      predict_lang = detect(data_str)\n",
    "    except:\n",
    "      predict_lang ='N/A'\n",
    "    return predict_lang\n",
    "\n",
    "#remove stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "def remove_stops(data_str):\n",
    "    comment = [word for word in data_str.split(\" \") if word not in stopwords]\n",
    "    return ' '.join(comment)\n",
    "\n",
    "#remove whitespace\n",
    "def remove_all_sapce(astring):\n",
    "  return \" \".join(astring.split())\n",
    "\n",
    "\n",
    "#clean the text \n",
    "def remove_features(data_str):\n",
    "    # compile regex\n",
    "    url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?')\n",
    "    punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    num_re = re.compile('(\\\\d+)')\n",
    "    alpha_num_re = re.compile(\"^[a-z0-9_.]+$\")\n",
    "    # convert to lowercase\n",
    "    data_str = data_str.lower()\n",
    "    # remove hyperlinks\n",
    "    data_str = url_re.sub(' ', data_str)\n",
    "    # remove puncuation\n",
    "    data_str = punc_re.sub(' ', data_str)\n",
    "    # remove numeric 'words'\n",
    "    data_str = num_re.sub(' ', data_str)\n",
    "    # remove non a-z 0-9 characters and words shorter than 3 characters\n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    for word in data_str.split():\n",
    "        if list_pos == 0:\n",
    "            if alpha_num_re.match(word) and len(word) > 2:\n",
    "                cleaned_str = word\n",
    "            else:\n",
    "                cleaned_str = ' '\n",
    "        else:\n",
    "            if alpha_num_re.match(word) and len(word) > 2:\n",
    "                cleaned_str = cleaned_str + ' ' + word\n",
    "            else:\n",
    "                cleaned_str += ' '\n",
    "        list_pos += 1\n",
    "    cleaned_str2 = remove_all_sapce(cleaned_str)\n",
    "    return cleaned_str2\n",
    "\n",
    "#check to see if a row only contains whitespace\n",
    "def check_blanks(data_str):\n",
    "    is_blank = str(data_str.isspace())\n",
    "    return is_blank\n",
    "\n",
    "#tokenize string\n",
    "def tokenize(aString):\n",
    "  a = word_tokenize(aString)\n",
    "  #only return values long enough\n",
    "  return a\n",
    "\n",
    "#extract part of speech\n",
    "def pos(tokenized_text):\n",
    "    sent_tag_list = pos_tag(tokenized_text) \n",
    "    aList = []\n",
    "    for word, tag in sent_tag_list:\n",
    "        tagToUse = ''\n",
    "        if tag.startswith('J'):\n",
    "            tagToUse= 'a'\n",
    "        elif tag.startswith('N'):\n",
    "            tagToUse= 'n'\n",
    "        elif tag.startswith('R'):\n",
    "            tagToUse= 'r'\n",
    "        elif tag.startswith('V'):\n",
    "            tagToUse= 'v'\n",
    "        else:\n",
    "            continue\n",
    "        aList.append((word, tagToUse))\n",
    "    return aList\n",
    "\n",
    "#lemmatize the commit comments  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(array_of_word_for_a_comment):\n",
    "  all_words_in_comment = []\n",
    "  for word in array_of_word_for_a_comment:\n",
    "    lemma = lemmatizer.lemmatize(word[0], pos=word[1])\n",
    "    if not lemma:\n",
    "      continue\n",
    "    all_words_in_comment.append([lemma,word[1]])  \n",
    "  return all_words_in_comment\n",
    "\n",
    "\n",
    "#calculate the sentiment \n",
    "def cal_score(array_of_lemma_tag_for_a_comment):\n",
    "    alist = [array_of_lemma_tag_for_a_comment]\n",
    "    totalScore = 0\n",
    "    count_words_included = 0\n",
    "    for word in array_of_lemma_tag_for_a_comment:\n",
    "        synset_forms = list(swn.senti_synsets(word[0], word[1]))\n",
    "        if not synset_forms:\n",
    "            continue\n",
    "        synset = synset_forms[0] \n",
    "        totalScore = totalScore + synset.pos_score() - synset.neg_score()\n",
    "        count_words_included = count_words_included +1\n",
    "    final_dec = ''\n",
    "    if count_words_included == 0:\n",
    "        final_dec = 'N/A'\n",
    "    elif totalScore == 0:\n",
    "        final_dec = 'Neu'        \n",
    "    elif totalScore/count_words_included < 0:\n",
    "        final_dec = 'Neg'\n",
    "    elif totalScore/count_words_included > 0:\n",
    "        final_dec = 'Pos'\n",
    "    return final_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We defined a number of function including: \n",
    "<ol>\n",
    "  <li><b>check_lang</b> determines  the language of text. It takes strings as input and output the language or n/a if it can't processes the text. This function is used given that in this project I will only target English comments</li>\n",
    "  <li><b>remove_stops</b> removes all stopwords from a sentence. Takes as input string of words and output string without stopwords. Stopwords carry no meaning for my sentiment analysis calculations</li>\n",
    "  <li><b>remove_all_sapce</b> removes extra white space from string. Takes as input string of words and output string with removed extra white space</li>\n",
    "  <li><b>remove_features</b> convert string to lowercase, remove hyperlinks, remove punctuations ,remove numeric 'words' and remove non a-z 0-9 characters and words. Takes as input string of words and outputs processed string. All these steps are needed to perform lemmatization and sentiment calculation </li>\n",
    "  <li><b>tokenize</b> tokenize string and return a token </li>\n",
    "  <li><b>pos</b> determine the part of speech for each word in a token (to be used for lemmatization and sentiment calculations). Takes a tokenized string as input and output tokenized string with each word part of speech . For example input = ('play', 'cards'), output = (('play','v'),('cards','n'))  </li>\n",
    "  <li><b>lemmatize</b> lemmatize each word in a token using wordnet. takes as input tokenize string of words and their part of speech and output the lemma of each word and its part of speech </li>\n",
    "  <li><b>cal_score</b> calculate the sentiment of tokenized string (sentence). takes as input lemmatize words in a sentence with their part of speech and outputs Neu, Neg, Pos or N/A (Neutral, Negative, Positive or Not Available) depending on the sentence sentiment </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Register functions in Spark\n",
    "check_lang_udf = udf(check_lang, StringType())\n",
    "remove_stops_udf = udf(remove_stops, StringType())\n",
    "remove_features_udf = udf(remove_features, StringType())\n",
    "tokenize_udf = udf(tokenize, ArrayType(StringType()))\n",
    "pos_udf = udf(pos,ArrayType(StructType([ StructField(\"word\", StringType(), False), StructField(\"tag\", StringType(), False)])))\n",
    "lemmatize_udf = udf(lemmatize,ArrayType(StructType([ StructField(\"lemma\", StringType(), False), StructField(\"tag\", StringType(), False)])))\n",
    "cal_score_udf = udf(cal_score,StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We register all the functions we previously defined in spark as users defined functions UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove null values\n",
    "data = commit_comments_df.fillna({'body':''})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all records that have empty comments section as they are not needed for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#keep only the english comments \n",
    "lang_df = data.withColumn(\"lang\", check_lang_udf(data[\"body\"]))\n",
    "en_df = lang_df.filter(lang_df[\"lang\"] == \"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new dataframe that has only commit comments written in English. This is a necessary  step given that in this project I'm only target English comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove noise\n",
    "rm_features_df = en_df.withColumn(\"feat_text\", remove_features_udf(en_df[\"body\"]))\n",
    "rm_features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new dataframe that contains processed commit comments (lowercase, with no hyperlinks, punctuations, numeric 'words' and non a-z 0-9 characters and words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove stopwords\n",
    "stops_df = rm_features_df.withColumn(\"no_stop\", remove_stops_udf(rm_features_df[\"feat_text\"]))\n",
    "stops_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new dataframe that contains commit comments with no stopwords as they carry no meaning for my sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove/filter empty \n",
    "stops_no_null = stops_df.filter(stops_df[\"no_stop\"] <> \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all records that have empty comments section as result of processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenize comments\n",
    "tokenized = stops_no_null.withColumn(\"tokenized\", tokenize_udf(stops_no_null[\"no_stop\"]))\n",
    "tokenized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new dataframe with tokenized  commit comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tag for part of speech\n",
    "pos_df = tokenized.withColumn(\"pos_out\", pos_udf(tokenized[\"tokenized\"]))\n",
    "pos_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We create a new dataframe with tokenized commit comments and each word part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lemmatize the tokens \n",
    "lem_df = pos_df.withColumn(\"lem_out\", lemmatize_udf(pos_df[\"pos_out\"]))\n",
    "lem_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new dataframe with a tokenized and lemmatize commit comments and each word part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate the sentiment\n",
    "try:\n",
    "  cal_df = lem_df.withColumn(\"score\", cal_score_udf(lem_df[\"lem_out\"]))\n",
    "except:#\n",
    "  sys.setrecursionlimit(2000)\n",
    "  cal_df = lem_df.withColumn(\"score\", cal_score_udf(lem_df[\"lem_out\"]))\n",
    "cal_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new dataframe with the sentiment score calculation for each commit comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creat persistent table for the commit_comments with sentiment to avoid running the code above before every analysis \n",
    "cal_df.write.saveAsTable(\"commit_comments_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Sentiment Analysis Results</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "desc commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "desc watchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "desc projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "desc commit_comments_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "/*How many project do we have per programming language in this sample?*/\n",
    "select p.language , count(*)\n",
    "from projects p\n",
    "group by p.language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample has 1000 project per programming language. The purpose of this query is to display how many projects are involved per programming language in this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "/*How many commit comments do we have per programming language ?*/\n",
    "SELECT  p.language language, count(*) Number_of_commit_comments\n",
    "FROM \n",
    "commit_comments_score cc inner join \n",
    "commits c on (cc.commit_id = c.id) inner join  \n",
    "projects p on (c.project_id = p.id) \n",
    "group by p.language\n",
    "order by p.language asc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "/*For every sentiment, show the percentage of programming language?*/\n",
    "SELECT p.language lan,\n",
    "       cc.score AS sentiment,\n",
    "       count(*)\n",
    "FROM commit_comments_score cc\n",
    "INNER JOIN commits c ON (cc.commit_id = c.id)\n",
    "INNER JOIN projects p ON (c.project_id = p.id)\n",
    "GROUP BY p.language,\n",
    "         sentiment\n",
    "ORDER BY lan DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the last two queries, most commit comments in the dataset are for C++, Java and Python. Given that each programing luggage has different number of processed commit comments, for this analysis it’s more appropriate to rely on averages than counts. \n",
    "\n",
    "\n",
    "The purpose of both queries is to examine the distribution of commit comments and determine the best analysis approach (counts vs averages).  \n",
    "<br><b>Sentiment labels meanings:</b></br> \n",
    "Neg = Negtive <br>\n",
    "Pos = Postive <br>\n",
    "N/A = unprocessed data<br>\n",
    "Neu = nutral <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "/*What is the % of each sentiment per programming language?*/\n",
    "select a.language as language, (a.sentiment_count/b.total_count)*100 as percentage_of_each_sentiment, a.sentiment\n",
    "from (SELECT  p.language language, count(*) sentiment_count, cc.score as sentiment\n",
    "FROM \n",
    "commit_comments_score cc inner join \n",
    "commits c on (cc.commit_id = c.id) inner join  \n",
    "projects p on (c.project_id = p.id) \n",
    "group by p.language, cc.score\n",
    "order by p.language asc , sentiment asc) a\n",
    "\n",
    "inner join \n",
    "(SELECT  p.language language, count(*) total_count\n",
    "FROM \n",
    "commit_comments_score cc inner join \n",
    "commits c on (cc.commit_id = c.id) inner join  \n",
    "projects p on (c.project_id = p.id) \n",
    "group by p.language\n",
    "order by p.language asc) b on (a.language = b.language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the graphs above we see how similar the sentiments are across programming languages. C has the most negative sentiment, followed by C++. For the most positive sentiments Python and Ruby scored the highest.\n",
    "\n",
    "The purpose of the query is to examine the percentage of commit comments sentiments  per each programming language. I want to see if comments sentiments differ in each language, and the result seems consistent (little difference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "/*What is the % of each sentiment per time of the day in which comments were posted?*/\n",
    "SELECT a.hr AS Hour,\n",
    "       (a.count_per_sentiment / b.total_count_per_hr) * 100 AS percentage_of_sentiment_per_hour,\n",
    "       a.sentiment AS sentiment\n",
    "FROM\n",
    "  (SELECT hour(cc.created_at) AS hr,\n",
    "          cc.score AS sentiment,\n",
    "          count(*) AS count_per_sentiment\n",
    "   FROM commit_comments_score cc\n",
    "   WHERE cc.created_at IS NOT NULL\n",
    "   GROUP BY hr,\n",
    "            sentiment\n",
    "   ORDER BY hr ASC)a\n",
    "INNER JOIN\n",
    "  (SELECT hour(cc.created_at) AS hr,\n",
    "          count(*) AS total_count_per_hr\n",
    "   FROM commit_comments_score cc\n",
    "   WHERE cc.created_at IS NOT NULL\n",
    "   GROUP BY hr\n",
    "   ORDER BY hr ASC)b ON (a.hr = b.hr)\n",
    "ORDER BY Hour ASC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "/*What is the % of each sentiment per the day of the week in which comments were posted?*/\n",
    "SELECT a.day_name AS day_name,\n",
    "       (a.count_per_sentiment / b.total_count_per_day_name) * 100 AS percentage_of_sentiment_per_day_name,\n",
    "       a.sentiment AS sentiment\n",
    "FROM\n",
    "  (SELECT date_format(cc.created_at,'EEEE') AS day_name,\n",
    "          cc.score AS sentiment,\n",
    "          count(*) AS count_per_sentiment\n",
    "   FROM commit_comments_score cc\n",
    "   WHERE cc.created_at IS NOT NULL\n",
    "   GROUP BY day_name,\n",
    "            sentiment\n",
    "   ORDER BY day_name ASC)a\n",
    "INNER JOIN\n",
    "  (SELECT date_format(cc.created_at,'EEEE') AS day_name,\n",
    "          count(*) AS total_count_per_day_name\n",
    "   FROM commit_comments_score cc\n",
    "   WHERE cc.created_at IS NOT NULL\n",
    "   GROUP BY day_name\n",
    "   ORDER BY day_name ASC)b ON (a.day_name = b.day_name)\n",
    "ORDER BY day_name ASC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "/*What is the % of each sentiment per month in which comments were posted?*/\n",
    "SELECT a.month AS month,\n",
    "       (a.count_per_sentiment / b.total_count_per_month) * 100 AS percentage_of_sentiment_per_month,\n",
    "       a.sentiment AS sentiment\n",
    "FROM\n",
    "  (SELECT month(cc.created_at) AS month,\n",
    "          cc.score AS sentiment,\n",
    "          count(*) AS count_per_sentiment\n",
    "   FROM commit_comments_score cc\n",
    "   WHERE cc.created_at IS NOT NULL\n",
    "   GROUP BY month,\n",
    "            sentiment\n",
    "   ORDER BY month ASC)a\n",
    "INNER JOIN\n",
    "  (SELECT month(cc.created_at) AS month,\n",
    "          count(*) AS total_count_per_month\n",
    "   FROM commit_comments_score cc\n",
    "   WHERE cc.created_at IS NOT NULL\n",
    "   GROUP BY month\n",
    "   ORDER BY month ASC)b ON (a.month = b.month)\n",
    "ORDER BY month ASC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accross all the time analysis above (last three queries) it seems that results are consistent accross all the different factors (ex, different time of day). The sentiments per each factor show little variations.\n",
    "\n",
    "The reason behind creating these queries is to investigate wither sentiment/commenters feelings are influenced by time related factors like the time of the day, day of the week and month, and based on the results there is little variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "/*check watchers distrubtion*/\n",
    "select count(*) project_per_bin, floor(a.stars/100) as bin\n",
    "from \n",
    "(\n",
    "SELECT \n",
    "        p2.id project, COUNT(*) AS stars\n",
    "    FROM\n",
    "        projects p2\n",
    "    INNER JOIN watchers w ON (p2.id = w.repo_id)\n",
    "    GROUP BY p2.id)a\n",
    "    \n",
    "group by bin\n",
    "order by bin asc  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous query notice how most of the data occurs between bin 0 and 7 , which means most of the repositories have 0 to 700 watchers. \n",
    "\n",
    "The reason behind this query is to determine a proper bin size to group the watchers in the next query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sql /* What is the % of sentiments in each watchers bin ?*/\n",
    "SELECT q1.bin as watchers_bin,\n",
    "       q1.sentiment as sentiment,\n",
    "       (q1.sentiment_sum/q2.total_sentiment_per_bin)*100 as percentage_of_sentiment\n",
    "   FROM\n",
    "  (SELECT d.sentiment sentiment,\n",
    "          floor(d.stars_per_project/50) bin,\n",
    "          sum(d.sentiment_count) sentiment_sum\n",
    "   FROM\n",
    "     (SELECT b.project project,\n",
    "             c.stars stars_per_project,\n",
    "             b.count_per_sentiment sentiment_count,\n",
    "             b.sentiment sentiment /*,\n",
    "    b.rank rank */\n",
    "      FROM\n",
    "        (SELECT a.*,\n",
    "                RANK() OVER (PARTITION BY a.project\n",
    "                             ORDER BY a.count_per_sentiment DESC) AS rank \n",
    "         FROM\n",
    "           ( SELECT p.id project,\n",
    "                    COUNT(*) AS count_per_sentiment,\n",
    "                    cc.score AS sentiment\n",
    "            FROM commit_comments_score cc\n",
    "            INNER JOIN commits c ON (cc.commit_id = c.id)\n",
    "            INNER JOIN projects p ON (c.project_id = p.id)\n",
    "            GROUP BY p.id,\n",
    "                     cc.score\n",
    "            ORDER BY p.id ASC , cc.score ASC) a ) b\n",
    "      INNER JOIN\n",
    "        (SELECT p2.id project,\n",
    "                COUNT(*) AS stars\n",
    "         FROM projects p2\n",
    "         INNER JOIN watchers w ON (p2.id = w.repo_id)\n",
    "         GROUP BY p2.id) c ON (b.project = c.project)\n",
    "      WHERE b.rank = 1) d\n",
    "   WHERE stars_per_project < 700 /*most of the data*/\n",
    "   GROUP BY floor(d.stars_per_project/50),\n",
    "            sentiment\n",
    "   ORDER BY bin ASC)q1\n",
    "INNER JOIN\n",
    "  (SELECT floor(stars_per_project/ 50) AS bin,\n",
    "          sum(sentiment_count) total_sentiment_per_bin\n",
    "   FROM\n",
    "     (SELECT b.project project,\n",
    "             c.stars stars_per_project,\n",
    "             b.count_per_sentiment sentiment_count,\n",
    "             b.sentiment sentiment /*,\n",
    "    b.rank rank */\n",
    "      FROM\n",
    "        (SELECT a.*,\n",
    "                RANK() OVER (PARTITION BY a.project\n",
    "                             ORDER BY a.count_per_sentiment DESC) AS rank \n",
    "         FROM\n",
    "           ( SELECT p.id project,\n",
    "                    COUNT(*) AS count_per_sentiment,\n",
    "                    cc.score AS sentiment\n",
    "            FROM commit_comments_score cc\n",
    "            INNER JOIN commits c ON (cc.commit_id = c.id)\n",
    "            INNER JOIN projects p ON (c.project_id = p.id)\n",
    "            GROUP BY p.id,\n",
    "                     cc.score\n",
    "            ORDER BY p.id ASC , cc.score ASC) a ) b\n",
    "      INNER JOIN\n",
    "        (SELECT p2.id project,\n",
    "                COUNT(*) AS stars\n",
    "         FROM projects p2\n",
    "         INNER JOIN watchers w ON (p2.id = w.repo_id)\n",
    "         GROUP BY p2.id) c ON (b.project = c.project)\n",
    "      WHERE b.rank = 1)\n",
    "   WHERE stars_per_project < 700\n",
    "   GROUP BY bin\n",
    "   ORDER BY bin ASC) AS q2 ON (q1.bin = q2.bin)\n",
    "   order by watchers_bin asc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this query for each project it finds the most occurring sentiment by counting each sentiment and ranking each, then group the data based on watchers count (bin) and report the percentage of each sentiment per watchers’ bin. \n",
    "The results show that generally repositories with higher watcher (<=450) have more positive comments. \n",
    "\n",
    "The reason behind this query is to see how the popularity of projects relate to sentiments and determine if this is something worth investigating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "SELECT DATEDIFF(c.created_at, cc.created_at) AS days,\n",
    "          count(*) as count_per_date_difference\n",
    "   FROM commit_comments_score cc\n",
    "   INNER JOIN commits c ON (cc.commit_id = c.id)\n",
    "   group by days\n",
    "   order by days desc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most commit comments occur with in the first 15 days after the commit\n",
    "\n",
    "The reason behind this query is to determine a proper bin size to group the days in the next query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "/*Show the commit comments sentiments based on every 5 days since the original commit*/\n",
    "SELECT a.sentiment,\n",
    "       a.days,\n",
    "       (a.count_per_sentiment/b.count_per_months_from_day_of_comment) * 100 AS count_per_sentiment_percentage_of_total_input_per_month\n",
    "FROM\n",
    "  (SELECT score sentiment,\n",
    "          floor(DATEDIFF(c.created_at, cc.created_at)/5) AS days,\n",
    "          count(*) count_per_sentiment\n",
    "   FROM commit_comments_score cc\n",
    "   INNER JOIN commits c ON (cc.commit_id = c.id)\n",
    "   WHERE floor(DATEDIFF(c.created_at, cc.created_at)/5) BETWEEN -2 AND 0 /*most of the data with in 15 day after commit*/\n",
    "   GROUP BY days,\n",
    "            sentiment\n",
    "   ORDER BY days ASC , sentiment ASC) a\n",
    "INNER JOIN\n",
    "  (SELECT floor(DATEDIFF(c.created_at, cc.created_at)/5) AS days,\n",
    "          count(*) AS count_per_months_from_day_of_comment\n",
    "   FROM commit_comments_score cc\n",
    "   INNER JOIN commits c ON (cc.commit_id = c.id)\n",
    "   WHERE floor(DATEDIFF(c.created_at, cc.created_at)/5) BETWEEN -2 AND 0 \n",
    "   GROUP BY days\n",
    "   ORDER BY days ASC)b ON (a.days = b.days)\n",
    "ORDER BY days desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the last query, the sentiment results are consistent for every five days with the first 15 days from the commit date. \n",
    "\n",
    "The reason behind this query is to examine wither the sentiments/commenters feelings are different over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>To summarize</h1>\n",
    "The introduction section of this notebook, highlights the key technologies and services used in this project and provides guidelines to help readers reproduce this work. The description of the dataset section provides information about each entity involved in this project to help the reader understand the dataset.  The wrangling section covers the key tasks and questions I tried to answer to prepare my dataset and select a sample. These tasks and questions are meant to help me explore the data, determine its structure and quality, and select a smaller sample. \n",
    "Importing the dataset section demonstrate how to import the sample selected into this notebook for analysis. It also has the code used to create Spark schemas and create the temporary tables. The preprocessing and sentiment calculation section starts with importing and downloading the needed Python packages (ex, NLTK, WordNetLemmatizer and detect)  and corpus(Wordnet and Sentiwordnet) to help preprocesses the data and determine the sentiment. All the functions needed to preprocess the data and calculate the sentiment are defined in this section. Each function is registered as UDF in spark to be used in a scalable way. The functions are then used on the sample dataset to produce the desired results (determine the sentiment of each commit comment). The Sentiment Analysis Results section presents the findings from applying the project method. The sentiments are similar across programming languages. The sentiments based on time related factors such as day of the week or time of day in which the commit comments was posted in, also show little difference. The result also shows that generally, repositories with higher watcher/popularity (<=450) have more positive comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]Pandarachalil, Rafeeque & Selvaraju, Sendhilkumar & G S, Mahalakshmi. (2014). Twitter Sentiment Analysis for Large-Scale Data: An Unsupervised Approach. Cognitive Computation. 7. 254-262. https://rd.springer.com/article/10.1007/s12559-014-9310-z\n",
    "\n",
    "[2] Dennis Hsu, Melody Moh, and Teng-Sheng Moh.(2017). Mining Frequency of Drug Side Effects over a Large Twitter Dataset Using Apache Spark. In Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017 (ASONAM '17), Jana Diesner, Elena Ferrari, and Guandong Xu (Eds.). ACM, New York, NY, USA, 915-924. DOI: https://doi.org/10.1145/3110025.3110110 \n",
    "\n",
    "[3]Kalliamvakou, E., Gousios, G., Blincoe, K. et al.(2016). An in-depth study of the promises and perils of mining GitHub. Empir Software Eng  21: 2035. https://doi.org/10.1007/s10664-015-9393-5\n",
    "\n",
    "[4]Georgios Gousios.(2013). The GHTorent dataset and tool suite. In Proceedings of the 10th Working Conference on Mining Software Repositories (MSR '13). IEEE Press, Piscataway, NJ, USA, 233-236.\n",
    "https://dl.acm.org/citation.cfm?id=2487085.2487132"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "name": "Bubshait-Mohammed-SentimentAnalysis",
  "notebookId": 3496444435337348,
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
